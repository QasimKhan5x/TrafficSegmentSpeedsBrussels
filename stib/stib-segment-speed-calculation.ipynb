{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_direction_id(group):\n",
    "    max_stop_sequence_row = group.loc[group['stop_sequence'].idxmax()]\n",
    "    group['direction_id'] = max_stop_sequence_row['stop_id']\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = gpd.read_file('../preprocessed_data/Stops.geojson')\n",
    "stops = stops.groupby(['line_id', 'direction']).apply(assign_direction_id)\n",
    "stops.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/bus_lines.txt\") as f:\n",
    "    lines = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/vehicle_positions_2024-08-27_09:02:21.json\") as f:\n",
    "    vehicle_positions = json.load(f)\n",
    "\n",
    "for line in vehicle_positions:\n",
    "    for instance in vehicle_positions[line][:]:\n",
    "        if instance['vehicle_positions'] is None:\n",
    "            vehicle_positions[line].remove(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_coordinates(geometry):\n",
    "    if geometry.geom_type == 'LineString':\n",
    "        # Reverse the coordinates of each point in the LineString\n",
    "        reversed_coords = [(y, x) for x, y in geometry.coords]\n",
    "        return LineString(reversed_coords)\n",
    "    else:\n",
    "        # Handle other geometry types if necessary\n",
    "        return geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = gpd.read_file('../data/segments.geojson')\n",
    "segments['geometry'] = segments['geometry'].apply(reverse_coordinates)\n",
    "segments = segments.to_crs(epsg=3812)\n",
    "segments = segments.drop(columns=['color'])\n",
    "segments['line_id'] = segments['line_id'].astype(int)\n",
    "segments['segment_length'] = segments['geometry'].length\n",
    "segments = segments.drop(columns=['id', 'distance', 'direction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_zero_distance(instance):\n",
    "    if len(instance['vehicle_positions']) == 0:\n",
    "        return instance\n",
    "    vehicle_positions = instance['vehicle_positions']\n",
    "    instance['vehicle_positions'] = [vehicle for vehicle in vehicle_positions if vehicle['distanceFromPoint'] > 0]\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_nearest_10_minutes(dt):\n",
    "    \"\"\"\n",
    "    Rounds a datetime object to the nearest 10 minutes.\n",
    "\n",
    "    :param dt: datetime object to be rounded\n",
    "    :return: datetime object rounded to the nearest 10 minutes\n",
    "    \"\"\"\n",
    "    # Calculate the number of minutes to add or subtract to round to the nearest 10 minutes\n",
    "    new_minute = (dt.minute // 10) * 10\n",
    "    remainder = dt.minute % 10\n",
    "\n",
    "    if remainder >= 5:\n",
    "        # If the remainder is 5 or more, round up to the next 10 minutes\n",
    "        dt = dt.replace(minute=new_minute, second=0, microsecond=0) + timedelta(minutes=10)\n",
    "    else:\n",
    "        # Otherwise, round down to the nearest 10 minutes\n",
    "        dt = dt.replace(minute=new_minute, second=0, microsecond=0)\n",
    "    dt = dt.replace(second=0, microsecond=0)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_with_flexible_direction(df, stops_line):\n",
    "    \"\"\"\n",
    "    Merge two DataFrames allowing for slight discrepancies in the 'directionId' column.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The first DataFrame containing 'pointId' and 'directionId'.\n",
    "    stops_line (pd.DataFrame): The second DataFrame containing 'stop_id', 'direction_id', 'stop_name', and 'stop_sequence'.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The merged DataFrame with a flexible match on 'directionId'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Original merge\n",
    "    df_merged = df.merge(\n",
    "        stops_line[[\"stop_id\", \"direction_id\", \"stop_name\", \"stop_sequence\"]],\n",
    "        left_on=[\"pointId\", \"directionId\"],\n",
    "        right_on=[\"stop_id\", \"direction_id\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "\n",
    "    # Merge with directionId + 1\n",
    "    df_merged_plus1 = df.merge(\n",
    "        stops_line[[\"stop_id\", \"direction_id\", \"stop_name\", \"stop_sequence\"]],\n",
    "        left_on=[\"pointId\", df[\"directionId\"] + 1],\n",
    "        right_on=[\"stop_id\", \"direction_id\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "\n",
    "    # Merge with directionId - 1\n",
    "    df_merged_minus1 = df.merge(\n",
    "        stops_line[[\"stop_id\", \"direction_id\", \"stop_name\", \"stop_sequence\"]],\n",
    "        left_on=[\"pointId\", df[\"directionId\"] - 1],\n",
    "        right_on=[\"stop_id\", \"direction_id\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "\n",
    "    # Concatenate the results\n",
    "    df_final = pd.concat([df_merged, df_merged_plus1, df_merged_minus1]).drop_duplicates()\n",
    "\n",
    "    # Drop the \"stop_id\" column if needed\n",
    "    df_final = df_final.drop(columns=[\"stop_id\"])\n",
    "\n",
    "    df_final.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df(instance, line_id):\n",
    "    line_id = int(line_id)\n",
    "    df = pd.DataFrame(instance[\"vehicle_positions\"])\n",
    "    df[\"directionId\"] = df[\"directionId\"].astype(int)\n",
    "    df[\"pointId\"] = df[\"pointId\"].astype(int)\n",
    "    # filter by line_id\n",
    "    stops_line = stops[stops[\"line_id\"] == line_id]\n",
    "    segments_line = segments[segments[\"line_id\"] == line_id]\n",
    "    # for cases like bus 71, where the directionId in data is +/-1 the direction_id in stops\n",
    "    num_rows_orig = df.shape[0]\n",
    "    df = merge_with_flexible_direction(df, stops_line)\n",
    "    num_rows_removed = num_rows_orig - df.shape[0]\n",
    "    df = df.merge(\n",
    "        segments_line[[\"start\", \"end\", \"segment_length\"]],\n",
    "        left_on=\"pointId\",\n",
    "        right_on=\"start\",\n",
    "        how=\"inner\",\n",
    "    ).drop(columns=[\"start\"])\n",
    "    df[\"rank\"] = df.groupby([\"directionId\", \"pointId\"])[\"distanceFromPoint\"].rank()\n",
    "    df[\"rank\"] = df[\"rank\"].astype(int)\n",
    "    return df, num_rows_removed\n",
    "\n",
    "\n",
    "def join_dataframes(df1, df2):\n",
    "    # Step 1: Perform exact match on directionId, stop_sequence, rank, and distanceFromPoint condition\n",
    "    # This means that the bus is in the same stop\n",
    "    exact_match = pd.merge(\n",
    "        df1,\n",
    "        df2,\n",
    "        on=[\"pointId\", \"directionId\", \"stop_sequence\", \"rank\"],\n",
    "        how=\"left\",\n",
    "        suffixes=(\"_df1\", \"_df2\"),\n",
    "    )\n",
    "    distance_from_point_condition = exact_match['distanceFromPoint_df2'] > exact_match['distanceFromPoint_df1']\n",
    "    # set row to null to where this is false\n",
    "    exact_match.loc[~distance_from_point_condition] = pd.NA\n",
    "    # add pointId_df1 and pointId_df2 columns to store the pointId values for both instances (they are the same)\n",
    "    exact_match.loc[~exact_match['stop_name_df2'].isna(), 'pointId_df1'] = exact_match.loc[~exact_match['stop_name_df2'].isna(), 'pointId']\n",
    "    exact_match.loc[~exact_match['stop_name_df2'].isna(), 'pointId_df2'] = exact_match.loc[~exact_match['stop_name_df2'].isna(), 'pointId']\n",
    "    # drop the pointId column since it is redundant\n",
    "    exact_match.drop(columns=['pointId'], inplace=True)\n",
    "\n",
    "    # Step 2: Perform match on directionId and (df1.stop_sequence + 1 = df2.stop_sequence)\n",
    "    df1_shifted = df1.copy()\n",
    "    df1_shifted[\"stop_sequence\"] += 1\n",
    "    shifted_match = pd.merge(\n",
    "        df1_shifted,\n",
    "        df2,\n",
    "        on=[\"directionId\", \"stop_sequence\", \"rank\"],\n",
    "        how=\"left\",\n",
    "        suffixes=(\"_df1\", \"_df2\"),\n",
    "    )\n",
    "\n",
    "    # Step 3: Combine results, prioritizing the exact match\n",
    "    combined = exact_match.combine_first(shifted_match)\n",
    "    \n",
    "    # Step 4: Drop any duplicates and keep the first occurrence based on original df1\n",
    "    combined = combined.drop_duplicates(\n",
    "        subset=[\"directionId\", \"stop_sequence\", \"rank\"], keep=\"first\"\n",
    "    )\n",
    "\n",
    "    # Step 5: Clean up the final DataFrame\n",
    "    final_result = combined.dropna()\n",
    "\n",
    "    return final_result\n",
    "\n",
    "\n",
    "def get_interval_pair(i, line_id):\n",
    "    instance1 = vehicle_positions[str(line_id)][i]\n",
    "    instance1 = filter_zero_distance(instance1)\n",
    "    num_rows_instance = len(instance1[\"vehicle_positions\"])\n",
    "    \n",
    "    instance2 = vehicle_positions[str(line_id)][i + 1]\n",
    "    instance2 = filter_zero_distance(instance2)\n",
    "    \n",
    "    ts1 = datetime.strptime(instance1[\"timestamp\"], \"%Y-%m-%d %H:%M:%S\")\n",
    "    ts2 = datetime.strptime(instance2[\"timestamp\"], \"%Y-%m-%d %H:%M:%S\")\n",
    "    interval_in_seconds = (ts2 - ts1).total_seconds()\n",
    "\n",
    "    if (\n",
    "        len(instance1[\"vehicle_positions\"]) == 0\n",
    "        or len(instance2[\"vehicle_positions\"]) == 0\n",
    "    ):\n",
    "        return None\n",
    "\n",
    "    df1, num_rows_removed = to_df(instance1, line_id)\n",
    "    df2, _ = to_df(instance2, line_id)\n",
    "    df3 = join_dataframes(df1, df2)\n",
    "\n",
    "    condition = df3[\"pointId_df1\"] == df3[\"pointId_df2\"]\n",
    "\n",
    "    # Calculate distance_traveled based on the condition\n",
    "    df3[\"distance_traveled\"] = condition * (\n",
    "        df3[\"distanceFromPoint_df2\"] - df3[\"distanceFromPoint_df1\"]\n",
    "    ) + (~condition) * (\n",
    "        (df3[\"segment_length_df1\"] - df3[\"distanceFromPoint_df1\"])\n",
    "        + df3[\"distanceFromPoint_df2\"]\n",
    "    )\n",
    "    # if the time interval is greater than 40 seconds, the distance traveled is 0\n",
    "    # because we missed a data point, so the time interval is unknown\n",
    "    if interval_in_seconds >= 40:\n",
    "        df3[\"distance_traveled\"] = 0\n",
    "    df3[\"speed\"] = df3[\"distance_traveled\"] / 20\n",
    "    df3[\"timestamp\"] = ts1\n",
    "\n",
    "    df3 = df3[\n",
    "        [\n",
    "            \"directionId\",\n",
    "            \"pointId_df1\",\n",
    "            \"end_df1\",\n",
    "            \"distanceFromPoint_df1\",\n",
    "            \"stop_name_df1\",\n",
    "            \"segment_length_df1\",\n",
    "            \"distanceFromPoint_df2\",\n",
    "            \"stop_name_df2\",\n",
    "            \"segment_length_df2\",\n",
    "            \"distance_traveled\",\n",
    "            \"speed\",\n",
    "            \"timestamp\"\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"df1\": df1,\n",
    "        \"df2\": df2,\n",
    "        \"df3\": df3,\n",
    "        \"num_rows_instance\": num_rows_instance,\n",
    "        \"num_rows_removed\": num_rows_removed\n",
    "    }\n",
    "\n",
    "\n",
    "def get_interval_pair_all_lines_all_instances(apply_filter=True):\n",
    "    total_df = None\n",
    "    total_rows_per_instance = 0\n",
    "    total_rows_removed = 0\n",
    "    for line in tqdm(vehicle_positions):\n",
    "        vehicle_positions_line = vehicle_positions[line]\n",
    "        line_id = int(line)\n",
    "        for i in trange(len(vehicle_positions_line) - 1):\n",
    "            result = get_interval_pair(i, line_id)\n",
    "            if result is None:\n",
    "                continue\n",
    "            df3 = result[\"df3\"]\n",
    "            total_rows_per_instance += result[\"num_rows_instance\"]\n",
    "            total_rows_removed += result[\"num_rows_removed\"]\n",
    "            if total_df is None:\n",
    "                total_df = df3\n",
    "            else:\n",
    "                if apply_filter:\n",
    "                    df3 = df3[(df3[\"speed\"] > 0) & (df3[\"speed\"] < 25)]\n",
    "                total_df = pd.concat([total_df, df3])\n",
    "    return total_df, total_rows_removed / total_rows_per_instance * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line_id = \"71\"\n",
    "# i = 1\n",
    "\n",
    "# instance1 = vehicle_positions[str(line_id)][i]\n",
    "# instance1 = filter_zero_distance(instance1)\n",
    "# df1 = to_df(instance1, line_id)\n",
    "\n",
    "# instance2 = vehicle_positions[str(line_id)][i + 1]\n",
    "# instance2 = filter_zero_distance(instance2)\n",
    "# df2 = to_df(instance2, line_id)\n",
    "\n",
    "# # df1, df2, df3 = get_interval_pair(25, \"66\")\n",
    "# df3 = join_dataframes(df1, df2)\n",
    "# df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf7efc26e4f4bb4810dc7423ffed2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e04c812daf4cb2ab7bfb201872d5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_all_intervals, perc_removed \u001b[38;5;241m=\u001b[39m \u001b[43mget_interval_pair_all_lines_all_instances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapply_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[49], line 151\u001b[0m, in \u001b[0;36mget_interval_pair_all_lines_all_instances\u001b[0;34m(apply_filter)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m apply_filter:\n\u001b[1;32m    150\u001b[0m                 df3 \u001b[38;5;241m=\u001b[39m df3[(df3[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeed\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (df3[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeed\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m25\u001b[39m)]\n\u001b[0;32m--> 151\u001b[0m             total_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtotal_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf3\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_df, total_rows_removed \u001b[38;5;241m/\u001b[39m total_rows_per_instance \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/libcity/lib/python3.8/site-packages/pandas/core/reshape/concat.py:385\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    370\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    372\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    373\u001b[0m     objs,\n\u001b[1;32m    374\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    383\u001b[0m )\n\u001b[0;32m--> 385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/libcity/lib/python3.8/site-packages/pandas/core/reshape/concat.py:616\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    612\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[1;32m    614\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[0;32m--> 616\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    620\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[0;32m~/miniconda3/envs/libcity/lib/python3.8/site-packages/pandas/core/internals/concat.py:242\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m    240\u001b[0m     fastpath \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m values\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 242\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_concatenate_join_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoin_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m     fastpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fastpath:\n",
      "File \u001b[0;32m~/miniconda3/envs/libcity/lib/python3.8/site-packages/pandas/core/internals/concat.py:581\u001b[0m, in \u001b[0;36m_concatenate_join_units\u001b[0;34m(join_units, copy)\u001b[0m\n\u001b[1;32m    578\u001b[0m has_none_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(unit\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m unit \u001b[38;5;129;01min\u001b[39;00m join_units)\n\u001b[1;32m    579\u001b[0m upcasted_na \u001b[38;5;241m=\u001b[39m _dtype_to_na_value(empty_dtype, has_none_blocks)\n\u001b[0;32m--> 581\u001b[0m to_concat \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    582\u001b[0m     ju\u001b[38;5;241m.\u001b[39mget_reindexed_values(empty_dtype\u001b[38;5;241m=\u001b[39mempty_dtype, upcasted_na\u001b[38;5;241m=\u001b[39mupcasted_na)\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units\n\u001b[1;32m    584\u001b[0m ]\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(to_concat) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;66;03m# Only one block, nothing to concatenate.\u001b[39;00m\n\u001b[1;32m    588\u001b[0m     concat_values \u001b[38;5;241m=\u001b[39m to_concat[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/libcity/lib/python3.8/site-packages/pandas/core/internals/concat.py:582\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    578\u001b[0m has_none_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(unit\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m unit \u001b[38;5;129;01min\u001b[39;00m join_units)\n\u001b[1;32m    579\u001b[0m upcasted_na \u001b[38;5;241m=\u001b[39m _dtype_to_na_value(empty_dtype, has_none_blocks)\n\u001b[1;32m    581\u001b[0m to_concat \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 582\u001b[0m     \u001b[43mju\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_reindexed_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mempty_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mempty_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupcasted_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupcasted_na\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units\n\u001b[1;32m    584\u001b[0m ]\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(to_concat) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;66;03m# Only one block, nothing to concatenate.\u001b[39;00m\n\u001b[1;32m    588\u001b[0m     concat_values \u001b[38;5;241m=\u001b[39m to_concat[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/libcity/lib/python3.8/site-packages/pandas/core/internals/concat.py:499\u001b[0m, in \u001b[0;36mJoinUnit.get_reindexed_values\u001b[0;34m(self, empty_dtype, upcasted_na)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    497\u001b[0m     fill_value \u001b[38;5;241m=\u001b[39m upcasted_na\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_is_valid_na_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mempty_dtype\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# note: always holds when self.block.dtype.kind == \"V\"\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         blk_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m blk_dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    504\u001b[0m             \u001b[38;5;66;03m# we want to avoid filling with np.nan if we are\u001b[39;00m\n\u001b[1;32m    505\u001b[0m             \u001b[38;5;66;03m# using None; we already know that we are all\u001b[39;00m\n\u001b[1;32m    506\u001b[0m             \u001b[38;5;66;03m# nulls\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/libcity/lib/python3.8/site-packages/pandas/core/internals/concat.py:440\u001b[0m, in \u001b[0;36mJoinUnit._is_valid_na_for\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    441\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mall\u001b[39m(is_valid_na_for_dtype(x, dtype) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m values\u001b[38;5;241m.\u001b[39mravel(order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/libcity/lib/python3.8/site-packages/pandas/_libs/properties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/libcity/lib/python3.8/site-packages/pandas/core/internals/concat.py:420\u001b[0m, in \u001b[0;36mJoinUnit.dtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 420\u001b[0m \u001b[38;5;129m@cache_readonly\u001b[39m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DtypeObj:\n\u001b[1;32m    422\u001b[0m     blk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_all_intervals, perc_removed = get_interval_pair_all_lines_all_instances(apply_filter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_intervals[df_all_intervals['speed'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the histogram with KDE using Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df_all_intervals['speed'], kde=True, bins=30)\n",
    "plt.title(\"Histogram and KDE of Speed\")\n",
    "plt.xlabel(\"Speed\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_intervals['speed'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_intervals['speed'] / 1000 * 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the boxplot using Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=df_all_intervals['speed'])\n",
    "plt.title(\"Boxplot of Speed\")\n",
    "plt.xlabel(\"Speed\")\n",
    "plt.xticks(ticks=range(int(df_all_intervals['speed'].min()), int(df_all_intervals['speed'].max()) + 1, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a violin plot using Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.violinplot(x=df_all_intervals['speed'])\n",
    "plt.title(\"Violin Plot of Speed\")\n",
    "plt.xlabel(\"Speed\")\n",
    "plt.xticks(ticks=range(int(df_all_intervals['speed'].min()), int(df_all_intervals['speed'].max()) + 1, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_intervals.rename(columns={\n",
    "    \"pointId_df1\": \"start\",\n",
    "    \"end_df1\": \"end\",\n",
    "}, inplace=True)\n",
    "df_all_intervals['start'] = df_all_intervals['start'].astype(int)\n",
    "df_all_intervals['end'] = df_all_intervals['end'].astype(int)\n",
    "\n",
    "df_all_intervals['timestamp'] = pd.to_datetime(df_all_intervals['timestamp'])\n",
    "\n",
    "# Step 2: Round 'timestamp' to the nearest 10 minutes\n",
    "df_all_intervals['datetime'] = df_all_intervals['timestamp'].dt.floor('10T')\n",
    "\n",
    "# Step 3: Group by 'start', 'end', and the rounded 'timestamp'\n",
    "grouped = df_all_intervals.groupby(['start', 'end', 'datetime'])\n",
    "\n",
    "# Step 4: Compute the mean and median speed\n",
    "df_10m_intervals = grouped['speed'].agg(['mean', 'median']).reset_index()\n",
    "df_10m_intervals.to_csv('../data/speeds_stib.csv', index=False)\n",
    "df_10m_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libcity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
